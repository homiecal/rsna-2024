{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"competition","sourceId":71549,"databundleVersionId":8561470},{"sourceType":"datasetVersion","sourceId":8974469,"datasetId":5347587,"databundleVersionId":9138886},{"sourceType":"datasetVersion","sourceId":8737101,"datasetId":5245104,"databundleVersionId":8891474},{"sourceType":"datasetVersion","sourceId":9883251,"datasetId":5428119,"databundleVersionId":10137669},{"sourceType":"datasetVersion","sourceId":9872404,"datasetId":5348137,"databundleVersionId":10125648},{"sourceType":"modelInstanceVersion","sourceId":161734,"databundleVersionId":10112548,"modelInstanceId":65863},{"sourceType":"modelInstanceVersion","sourceId":157931,"databundleVersionId":10070886,"modelInstanceId":65863},{"sourceType":"modelInstanceVersion","sourceId":78363,"databundleVersionId":9138930,"modelInstanceId":65863},{"sourceType":"modelInstanceVersion","sourceId":164040,"databundleVersionId":10136717,"modelInstanceId":65863},{"sourceType":"modelInstanceVersion","sourceId":156724,"databundleVersionId":10057689,"modelInstanceId":65863}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport pydicom\nimport numpy as np\nimport os\nimport glob\nfrom tqdm import tqdm\nimport warnings\nDATA_PATH = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/'\nWORKING_DIR = '/kaggle/input/rsna-2024-train/'\n\nimport pydicom\nimport numpy as np\nfrom PIL import Image\nfrom random import randint\nfrom torch.utils.data import DataLoader\n\nimport torch\n\nSUBMISSION=True\n\nLUMBAR_LABEL_MAPPING = {\"l1_l2\":0,\"l2_l3\":1,\"l3_l4\":2,\"l4_l5\":3,\"l5_s1\":4}\nLUMBAR_INT_MAPPING = {value: key for key, value in LUMBAR_LABEL_MAPPING.items()}\nSET_TYPE = 'train'\nDATASET_SELECTION = 'axial'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T10:43:15.888508Z","iopub.execute_input":"2024-11-12T10:43:15.888885Z","iopub.status.idle":"2024-11-12T10:43:15.896820Z","shell.execute_reply.started":"2024-11-12T10:43:15.888858Z","shell.execute_reply":"2024-11-12T10:43:15.895864Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"MODEL_DIR = \"/kaggle/input/rsna-yolo-v1/pytorch/best_20240716/5/new_sagittal_20241110.pt\" if DATASET_SELECTION=='sagittal' else \"/kaggle/input/rsna-yolo-v1/pytorch/best_20240716/6/best_axial_20241112.pt\"\nIMAGE_DIR = \"/kaggle/working/yolo_set/images/inference/\"\nprocess_test = False","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:43:16.100180Z","iopub.execute_input":"2024-11-12T10:43:16.100564Z","iopub.status.idle":"2024-11-12T10:43:16.105367Z","shell.execute_reply.started":"2024-11-12T10:43:16.100534Z","shell.execute_reply":"2024-11-12T10:43:16.104337Z"},"trusted":true},"outputs":[],"execution_count":101},{"cell_type":"code","source":"!pip install ultralytics # '8.3.23'\nimport os \nos.environ['WANDB_MODE'] = 'disabled'\nos.environ['WANDB_DISABLED'] = 'true'\n\nfrom ultralytics import YOLO, settings\nsettings.update({\"wandb\": False})","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:17.413664Z","iopub.execute_input":"2024-11-12T10:13:17.414441Z","iopub.status.idle":"2024-11-12T10:13:29.761731Z","shell.execute_reply.started":"2024-11-12T10:13:17.414410Z","shell.execute_reply":"2024-11-12T10:13:29.760704Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: ultralytics in /opt/conda/lib/python3.10/site-packages (8.3.29)\nRequirement already satisfied: numpy>=1.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.10.0.82)\nRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.5.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (6.0.1)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.11.4)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.1.2)\nRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.16.2)\nRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.2.1)\nRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.12.2)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.0.11)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2024.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"set_desc = pd.read_csv(DATA_PATH+SET_TYPE+'_series_descriptions.csv')","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:29.764172Z","iopub.execute_input":"2024-11-12T10:13:29.765018Z","iopub.status.idle":"2024-11-12T10:13:29.784148Z","shell.execute_reply.started":"2024-11-12T10:13:29.764979Z","shell.execute_reply":"2024-11-12T10:13:29.783298Z"},"trusted":true},"outputs":[],"execution_count":66},{"cell_type":"code","source":"set_desc.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:29.785248Z","iopub.execute_input":"2024-11-12T10:13:29.785519Z","iopub.status.idle":"2024-11-12T10:13:29.795771Z","shell.execute_reply.started":"2024-11-12T10:13:29.785489Z","shell.execute_reply":"2024-11-12T10:13:29.794890Z"},"trusted":true},"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"   study_id   series_id series_description\n0   4003253   702807833   Sagittal T2/STIR\n1   4003253  1054713880        Sagittal T1\n2   4003253  2448190387           Axial T2\n3   4646740  3201256954           Axial T2\n4   4646740  3486248476        Sagittal T1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>study_id</th>\n      <th>series_id</th>\n      <th>series_description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4003253</td>\n      <td>702807833</td>\n      <td>Sagittal T2/STIR</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4003253</td>\n      <td>1054713880</td>\n      <td>Sagittal T1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4003253</td>\n      <td>2448190387</td>\n      <td>Axial T2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4646740</td>\n      <td>3201256954</td>\n      <td>Axial T2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4646740</td>\n      <td>3486248476</td>\n      <td>Sagittal T1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":67},{"cell_type":"code","source":"sagittal_t1 = set_desc[set_desc['series_description']=='Sagittal T1']\nsagittal = set_desc[set_desc['series_description'].isin(['Sagittal T2/STIR','Sagittal T1'])]\naxial = set_desc[set_desc['series_description'].isin(['Axial T2'])]","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:29.798153Z","iopub.execute_input":"2024-11-12T10:13:29.799005Z","iopub.status.idle":"2024-11-12T10:13:29.809216Z","shell.execute_reply.started":"2024-11-12T10:13:29.798961Z","shell.execute_reply":"2024-11-12T10:13:29.808238Z"},"trusted":true},"outputs":[],"execution_count":68},{"cell_type":"code","source":"datasets = {'sagittal':sagittal, 'axial':axial,  'sagittal_t1':sagittal_t1, 'all':set_desc}","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:29.810647Z","iopub.execute_input":"2024-11-12T10:13:29.811296Z","iopub.status.idle":"2024-11-12T10:13:29.815975Z","shell.execute_reply.started":"2024-11-12T10:13:29.811263Z","shell.execute_reply":"2024-11-12T10:13:29.815124Z"},"trusted":true},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":"### Get study_id_meta","metadata":{}},{"cell_type":"code","source":"def get_study_id_meta(set_desc, set_type: str = 'train'):\n    \"\"\"\n    {\n        study_id: {\n            folder_path: ...\n            series_ids: []\n            series_descriptions: []\n        }\n    }\n    \"\"\"\n    \n    if set_type not in (\"train\",\"test\"):\n        raise Exception(f\"set_type must be equal to 'train' or 'test', set_type = '{set_type}''\")\n    # First aggregate the descriptions, so we have a list of series descriptions, sort the dataframe by series_id to maintain order mapping\n    set_desc_sorted = set_desc.sort_values(['study_id','series_id']).groupby('study_id').agg(list) \n    test_study_ids = set_desc_sorted.index.tolist()\n\n    # All file ids\n    test_study_ids_path = [int(i.split(\"/\")[-1]) for i in glob.glob(os.path.join(DATA_PATH,f'{set_type}_images/*'))]\n\n    # Check all study_ids exist if they don't then delete from inference pack\n    for ndx, id in enumerate(test_study_ids):\n        if id not in test_study_ids_path:\n            print(f\"study_id {id} not on file\")\n            del test_study_ids[ndx]\n\n    # Now create meta information, filtering on index\n    study_id_meta = {}\n    for study_id in test_study_ids:\n        folder_path = os.path.join(DATA_PATH,f'{set_type}_images/{study_id}')\n        study_id_meta.update({study_id: {\n                                    \"folder_path\": folder_path,\n                                    \"series_id_files\": set_desc_sorted.loc[study_id].series_id,\n                                    \"series_descriptions\": set_desc_sorted.loc[study_id].series_description\n                                    }\n                             })\n        \n    return study_id_meta","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:29.817349Z","iopub.execute_input":"2024-11-12T10:13:29.818134Z","iopub.status.idle":"2024-11-12T10:13:29.831353Z","shell.execute_reply.started":"2024-11-12T10:13:29.818108Z","shell.execute_reply":"2024-11-12T10:13:29.830423Z"},"trusted":true},"outputs":[],"execution_count":70},{"cell_type":"code","source":"study_id_meta = get_study_id_meta(datasets[DATASET_SELECTION], SET_TYPE)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:29.832230Z","iopub.execute_input":"2024-11-12T10:13:29.832500Z","iopub.status.idle":"2024-11-12T10:13:30.184756Z","shell.execute_reply.started":"2024-11-12T10:13:29.832478Z","shell.execute_reply":"2024-11-12T10:13:30.183993Z"},"trusted":true},"outputs":[],"execution_count":71},{"cell_type":"code","source":"len(study_id_meta)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:30.186043Z","iopub.execute_input":"2024-11-12T10:13:30.186435Z","iopub.status.idle":"2024-11-12T10:13:30.192606Z","shell.execute_reply.started":"2024-11-12T10:13:30.186400Z","shell.execute_reply":"2024-11-12T10:13:30.191720Z"},"trusted":true},"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"1975"},"metadata":{}}],"execution_count":72},{"cell_type":"markdown","source":"### Datasets (Region of interest derived from coordinates) \n\nComponents:\n- CandidateList: list of instances with valid coordinates and their labelling info\n- Region of interest calculation: crude bounding box calculation from coordinates\n    - cache the data\n- Return: Image, Label (severity)","metadata":{}},{"cell_type":"code","source":"from collections import namedtuple\nimport functools\nimport datetime\nfrom torch.utils.data import Dataset\nimport random\ncandidate_info_tuple = namedtuple(\n    'candidate_info_tuple',\n    'row_id, study_id, series_id, instance_number, centre_xy, severity, img_path, width_bbox, height_bbox'\n)\nSEVERITY_MAPPING = {\"Normal/Mild\":0,\"Moderate\":1,\"Severe\":2}\nSEVERITY_WEIGHTING = {0:1.0,1:2.0,2:4.0}","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:30.193812Z","iopub.execute_input":"2024-11-12T10:13:30.194104Z","iopub.status.idle":"2024-11-12T10:13:30.204696Z","shell.execute_reply.started":"2024-11-12T10:13:30.194079Z","shell.execute_reply":"2024-11-12T10:13:30.203910Z"},"trusted":true},"outputs":[],"execution_count":73},{"cell_type":"code","source":"@functools.lru_cache(1, typed=True)\ndef get_candidate_list(selection='all'):\n    \n    candidate_dataset = datasets[selection]\n    \n    candidate_list = []\n    \n    for ndx, instance in enumerate(candidate_dataset.itertuples(name='Candidate')):\n        \n        try:\n            img_path = f\"{DATA_PATH}/{SET_TYPE}_images/{instance.study_id}/{instance.series_id}/\"\n\n            images = glob.glob(f\"{img_path}/*.dcm\")\n            \n            for inst_path in images:\n                \n                instance_number = inst_path.split('/')[-1].replace('.dcm', '')\n\n                candidate_list.append(\n                    candidate_info_tuple(\n                        row_id = None,\n                        study_id = instance.study_id, \n                        series_id = instance.series_id, \n                        instance_number = instance_number, \n                        centre_xy = tuple(), \n                        severity = None, \n                        img_path = inst_path,\n                        width_bbox=None,\n                        height_bbox=None\n\n                    )\n                )\n            \n            if not os.path.exists(img_path):\n                print(f\"Path not found {img_path}\")\n                continue\n\n            if ndx % 10000 == 0 and ndx !=0:\n                print(f\"{datetime.datetime.now()}, {ndx} records processed\")\n            \n\n        except Exception as e:\n            print(instance)\n            raise e\n\n    return candidate_list","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:30.208039Z","iopub.execute_input":"2024-11-12T10:13:30.208390Z","iopub.status.idle":"2024-11-12T10:13:30.219042Z","shell.execute_reply.started":"2024-11-12T10:13:30.208357Z","shell.execute_reply":"2024-11-12T10:13:30.218233Z"},"trusted":true},"outputs":[],"execution_count":74},{"cell_type":"code","source":"@functools.lru_cache(1, typed=True)\ndef get_image(path):\n    return pydicom.dcmread(path)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:30.220338Z","iopub.execute_input":"2024-11-12T10:13:30.220878Z","iopub.status.idle":"2024-11-12T10:13:30.229111Z","shell.execute_reply.started":"2024-11-12T10:13:30.220846Z","shell.execute_reply":"2024-11-12T10:13:30.228237Z"},"trusted":true},"outputs":[],"execution_count":75},{"cell_type":"code","source":"candidate_list = get_candidate_list(DATASET_SELECTION)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:30.230365Z","iopub.execute_input":"2024-11-12T10:13:30.230705Z","iopub.status.idle":"2024-11-12T10:13:32.815220Z","shell.execute_reply.started":"2024-11-12T10:13:30.230673Z","shell.execute_reply":"2024-11-12T10:13:32.814212Z"},"trusted":true},"outputs":[],"execution_count":76},{"cell_type":"code","source":"candidate_list[1]","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:32.816435Z","iopub.execute_input":"2024-11-12T10:13:32.816783Z","iopub.status.idle":"2024-11-12T10:13:32.823012Z","shell.execute_reply.started":"2024-11-12T10:13:32.816752Z","shell.execute_reply":"2024-11-12T10:13:32.822091Z"},"trusted":true},"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"candidate_info_tuple(row_id=None, study_id=4003253, series_id=2448190387, instance_number='18', centre_xy=(), severity=None, img_path='/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification//train_images/4003253/2448190387/18.dcm', width_bbox=None, height_bbox=None)"},"metadata":{}}],"execution_count":77},{"cell_type":"markdown","source":"Understanding the image characeristics to see if there is some way we can inform the bounding box width/heights.","metadata":{}},{"cell_type":"code","source":"import cv2\n\nIMG_RESIZE = (416, 416)\n\ndef yolo_preprocess(img: np.array, img_resize: tuple): \n    \"\"\"\n    \"\"\"\n\n    # Normalize to the range [0, 255] using the actual max value\n    image_2d = img.astype(float)\n    image_2d = (np.maximum(image_2d, 0) / image_2d.max()) * 255.0\n    image_2d = np.uint8(image_2d)\n    \n    # Resize image\n    resized_image = cv2.resize(image_2d, IMG_RESIZE, interpolation=cv2.INTER_LINEAR)\n    \n    return resized_image\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:32.824205Z","iopub.execute_input":"2024-11-12T10:13:32.824496Z","iopub.status.idle":"2024-11-12T10:13:32.832918Z","shell.execute_reply.started":"2024-11-12T10:13:32.824473Z","shell.execute_reply":"2024-11-12T10:13:32.832069Z"},"trusted":true},"outputs":[],"execution_count":78},{"cell_type":"code","source":"class LumbarYoloInference(Dataset):\n    \n    def __init__(self,\n                val_stride=0,\n                val_set=False,\n                study_id=None,\n                candidate_list=None,\n                transform=None,\n                sample: int | None = None,\n                rand_bbox: bool = False,\n                sorted_candidate_list=None, \n                save_to_drive: bool = False):\n        \n        self.sample = sample\n        self.rand_bbox = rand_bbox\n        self.save_to_drive = save_to_drive\n        \n        if candidate_list:\n            self.candidate_list = candidate_list.copy()\n        else:\n            self.candidate_list = get_candidate_list().copy()\n        \n        # Default to a stratified sampled of candidates by study_id (option to provide custom sorted list on other\n        # stratifications).\n        if sorted_candidate_list:\n            self.sorted_candidate_list = sorted_candidate_list.copy()\n        else:\n            study_ids = np.unique([x.study_id for x in self.candidate_list])\n            self.sorted_candidate_list = np.random.shuffle(study_ids)\n        \n        if study_id:\n            self.candidate_list = [x for x in self.candidate_list if x.study_id == study_id]\n        \n        if val_set:\n            assert val_stride > 0, val_stride\n            val_study_ids = self.sorted_candidate_list[::val_stride]\n            self.candidate_list = [x for x in self.candidate_list if str(x.study_id) in val_study_ids]\n            assert self.candidate_list\n        elif val_stride > 0:\n            del self.sorted_candidate_list[::val_stride]                        \n            self.candidate_list = [x for x in self.candidate_list if str(x.study_id) in self.sorted_candidate_list]            \n            assert self.candidate_list\n    \n        self.transform = transform\n    \n    def __len__(self):\n        if self.sample:\n            return min(self.sample, len(self.candidate_list))\n        else:\n            return len(self.candidate_list)\n    \n    def __getitem__(self, ndx):\n        candidate = self.candidate_list[ndx]\n\n        # Get full image\n        image = get_image(candidate.img_path)\n\n        resized_image = yolo_preprocess(image.pixel_array, IMG_RESIZE)\n\n        # Pass in defaults so can be used with DataLoader\n        candidate = candidate._replace(\n            row_id=candidate.row_id or \"\",\n            severity=candidate.severity or 0,\n            centre_xy=candidate.centre_xy or (0, 0),\n            width_bbox=candidate.width_bbox or 0,\n            height_bbox=candidate.height_bbox or 0\n        )\n        return resized_image, str(candidate.study_id), str(candidate.series_id), candidate.instance_number\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:32.834289Z","iopub.execute_input":"2024-11-12T10:13:32.834604Z","iopub.status.idle":"2024-11-12T10:13:32.848320Z","shell.execute_reply.started":"2024-11-12T10:13:32.834580Z","shell.execute_reply":"2024-11-12T10:13:32.847466Z"},"trusted":true},"outputs":[],"execution_count":79},{"cell_type":"code","source":"# candidate_list = [i for i in candidate_list if i.study_id == 4003253 and i.series_id == 702807833]\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:32.849362Z","iopub.execute_input":"2024-11-12T10:13:32.849679Z","iopub.status.idle":"2024-11-12T10:13:32.862381Z","shell.execute_reply.started":"2024-11-12T10:13:32.849655Z","shell.execute_reply":"2024-11-12T10:13:32.861532Z"},"trusted":true},"outputs":[],"execution_count":80},{"cell_type":"code","source":"InferenceSet = LumbarYoloInference(candidate_list=candidate_list)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:32.863468Z","iopub.execute_input":"2024-11-12T10:13:32.865322Z","iopub.status.idle":"2024-11-12T10:13:32.890022Z","shell.execute_reply.started":"2024-11-12T10:13:32.865290Z","shell.execute_reply":"2024-11-12T10:13:32.888976Z"},"trusted":true},"outputs":[],"execution_count":81},{"cell_type":"code","source":"yolo_dirs = [\"/kaggle/working/yolo_set/images/inference/\"]\nfor d in yolo_dirs:\n    if os.path.exists(d):\n        continue\n    else:\n        os.makedirs(d)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:32.891361Z","iopub.execute_input":"2024-11-12T10:13:32.891695Z","iopub.status.idle":"2024-11-12T10:13:32.897115Z","shell.execute_reply.started":"2024-11-12T10:13:32.891666Z","shell.execute_reply":"2024-11-12T10:13:32.896217Z"},"trusted":true},"outputs":[],"execution_count":82},{"cell_type":"code","source":"from concurrent.futures import ThreadPoolExecutor\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:32.898272Z","iopub.execute_input":"2024-11-12T10:13:32.898563Z","iopub.status.idle":"2024-11-12T10:13:32.906093Z","shell.execute_reply.started":"2024-11-12T10:13:32.898540Z","shell.execute_reply":"2024-11-12T10:13:32.905204Z"},"trusted":true},"outputs":[],"execution_count":83},{"cell_type":"code","source":"InferenceLoader = DataLoader(dataset=InferenceSet, batch_size=64, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:32.907147Z","iopub.execute_input":"2024-11-12T10:13:32.907439Z","iopub.status.idle":"2024-11-12T10:13:32.915726Z","shell.execute_reply.started":"2024-11-12T10:13:32.907416Z","shell.execute_reply":"2024-11-12T10:13:32.914928Z"},"trusted":true},"outputs":[],"execution_count":84},{"cell_type":"code","source":"InferenceSet[0]","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:32.916843Z","iopub.execute_input":"2024-11-12T10:13:32.917169Z","iopub.status.idle":"2024-11-12T10:13:32.950935Z","shell.execute_reply.started":"2024-11-12T10:13:32.917136Z","shell.execute_reply":"2024-11-12T10:13:32.950077Z"},"trusted":true},"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"(array([[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 2, ..., 3, 1, 1],\n        [0, 1, 3, ..., 4, 1, 1],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n '4003253',\n '2448190387',\n '12')"},"metadata":{}}],"execution_count":85},{"cell_type":"code","source":"def save_image(image, study_id, series_id, instance_number):    \n    image_key = \"_\".join([str(study_id),str(series_id), str(int(instance_number))]) \n    img_path = f'/kaggle/working/yolo_set/images/inference/{image_key}.png'\n    \n    # Save image\n    cv2.imwrite(img_path,image.numpy())","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:32.951842Z","iopub.execute_input":"2024-11-12T10:13:32.952069Z","iopub.status.idle":"2024-11-12T10:13:32.957057Z","shell.execute_reply.started":"2024-11-12T10:13:32.952049Z","shell.execute_reply":"2024-11-12T10:13:32.956225Z"},"trusted":true},"outputs":[],"execution_count":86},{"cell_type":"code","source":"img_path = \"/kaggle/working/yolo_set/images/inference/4003253_702807833_12.png\"","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:32.958127Z","iopub.execute_input":"2024-11-12T10:13:32.958472Z","iopub.status.idle":"2024-11-12T10:13:32.965796Z","shell.execute_reply.started":"2024-11-12T10:13:32.958442Z","shell.execute_reply":"2024-11-12T10:13:32.965126Z"},"trusted":true},"outputs":[],"execution_count":87},{"cell_type":"code","source":"for ndx, batch in enumerate(InferenceLoader):\n    \n    images, study_id, series_id, instance_number = batch\n\n    with ThreadPoolExecutor(max_workers = 8) as executor:\n        executor.map(save_image, images, study_id, series_id, instance_number)\n            \n    if (ndx*64)%1280 == 0:\n        print(f\"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, Processed {ndx*64} images.\")\n    \n    if ndx==len(InferenceLoader):\n        print(f\"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, End\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:13:32.966974Z","iopub.execute_input":"2024-11-12T10:13:32.967314Z","iopub.status.idle":"2024-11-12T10:23:10.177924Z","shell.execute_reply.started":"2024-11-12T10:13:32.967284Z","shell.execute_reply":"2024-11-12T10:23:10.176737Z"},"trusted":true},"outputs":[{"name":"stdout","text":"2024-11-12 10:13:34, Processed 0 images.\n2024-11-12 10:13:42, Processed 1280 images.\n2024-11-12 10:13:52, Processed 2560 images.\n2024-11-12 10:14:01, Processed 3840 images.\n2024-11-12 10:14:10, Processed 5120 images.\n2024-11-12 10:14:21, Processed 6400 images.\n2024-11-12 10:14:29, Processed 7680 images.\n2024-11-12 10:14:36, Processed 8960 images.\n2024-11-12 10:14:46, Processed 10240 images.\n2024-11-12 10:14:54, Processed 11520 images.\n2024-11-12 10:15:04, Processed 12800 images.\n2024-11-12 10:15:12, Processed 14080 images.\n2024-11-12 10:15:21, Processed 15360 images.\n2024-11-12 10:15:30, Processed 16640 images.\n2024-11-12 10:15:39, Processed 17920 images.\n2024-11-12 10:15:47, Processed 19200 images.\n2024-11-12 10:15:55, Processed 20480 images.\n2024-11-12 10:16:06, Processed 21760 images.\n2024-11-12 10:16:16, Processed 23040 images.\n2024-11-12 10:16:24, Processed 24320 images.\n2024-11-12 10:16:33, Processed 25600 images.\n2024-11-12 10:16:42, Processed 26880 images.\n2024-11-12 10:16:51, Processed 28160 images.\n2024-11-12 10:16:59, Processed 29440 images.\n2024-11-12 10:17:09, Processed 30720 images.\n2024-11-12 10:17:18, Processed 32000 images.\n2024-11-12 10:17:27, Processed 33280 images.\n2024-11-12 10:17:35, Processed 34560 images.\n2024-11-12 10:17:44, Processed 35840 images.\n2024-11-12 10:17:56, Processed 37120 images.\n2024-11-12 10:18:06, Processed 38400 images.\n2024-11-12 10:18:15, Processed 39680 images.\n2024-11-12 10:18:24, Processed 40960 images.\n2024-11-12 10:18:34, Processed 42240 images.\n2024-11-12 10:18:42, Processed 43520 images.\n2024-11-12 10:18:52, Processed 44800 images.\n2024-11-12 10:19:00, Processed 46080 images.\n2024-11-12 10:19:10, Processed 47360 images.\n2024-11-12 10:19:21, Processed 48640 images.\n2024-11-12 10:19:28, Processed 49920 images.\n2024-11-12 10:19:37, Processed 51200 images.\n2024-11-12 10:19:45, Processed 52480 images.\n2024-11-12 10:19:54, Processed 53760 images.\n2024-11-12 10:20:04, Processed 55040 images.\n2024-11-12 10:20:13, Processed 56320 images.\n2024-11-12 10:20:21, Processed 57600 images.\n2024-11-12 10:20:29, Processed 58880 images.\n2024-11-12 10:20:39, Processed 60160 images.\n2024-11-12 10:20:51, Processed 61440 images.\n2024-11-12 10:21:00, Processed 62720 images.\n2024-11-12 10:21:09, Processed 64000 images.\n2024-11-12 10:21:18, Processed 65280 images.\n2024-11-12 10:21:29, Processed 66560 images.\n2024-11-12 10:21:38, Processed 67840 images.\n2024-11-12 10:21:48, Processed 69120 images.\n2024-11-12 10:21:56, Processed 70400 images.\n2024-11-12 10:22:05, Processed 71680 images.\n2024-11-12 10:22:17, Processed 72960 images.\n2024-11-12 10:22:27, Processed 74240 images.\n2024-11-12 10:22:37, Processed 75520 images.\n2024-11-12 10:22:47, Processed 76800 images.\n2024-11-12 10:22:58, Processed 78080 images.\n2024-11-12 10:23:07, Processed 79360 images.\n","output_type":"stream"}],"execution_count":88},{"cell_type":"markdown","source":"## Inference\n\nInference on YOLO for sagittal.","metadata":{}},{"cell_type":"code","source":"import glob\n# Load a model\nmodel = YOLO(MODEL_DIR)  # pretrained YOLOv8n model\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel.to(device)\n\n# Run batched inference on a list of images\nfiles_for_inference = sorted(glob.glob(f\"{IMAGE_DIR}*\"))\nresults = model.predict(files_for_inference,task='detect',iou=0.8,stream=True)  # return a list of Results objects\n\nif process_test:\n    # Process results list\n    for result in results:\n        boxes = result.boxes  # Boxes object for bounding box outputs\n        masks = result.masks  # Masks object for segmentation masks outputs\n        keypoints = result.keypoints  # Keypoints object for pose outputs\n        probs = result.probs  # Probs object for classification outputs\n        obb = result.obb  # Oriented boxes object for OBB outputs\n    #     result.show()  # display to screen\n        path = result.path.split(\"/\")[-1].replace(\".png\",\"\")","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:43:29.472922Z","iopub.execute_input":"2024-11-12T10:43:29.473314Z","iopub.status.idle":"2024-11-12T10:43:30.709692Z","shell.execute_reply.started":"2024-11-12T10:43:29.473282Z","shell.execute_reply":"2024-11-12T10:43:30.708669Z"},"trusted":true},"outputs":[],"execution_count":102},{"cell_type":"code","source":"import re\ndef clean_newlines(input_string):\n    # Replace multiple newlines with a single newline\n    cleaned_string = re.sub(r'\\n+', '\\n', input_string)\n    # If there are newlines at the end, remove them\n    cleaned_string = re.sub(r'\\n+$', '', cleaned_string)\n    return cleaned_string.split(\"\\n\")\n\n# Function to extract the numeric values from the filename\ndef extract_numbers(file_path):\n    filename = file_path.split('/')[-1]\n    numbers = re.findall(r'\\d+', filename)\n    return list(map(int, numbers))\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:43:30.711355Z","iopub.execute_input":"2024-11-12T10:43:30.711651Z","iopub.status.idle":"2024-11-12T10:43:30.717585Z","shell.execute_reply.started":"2024-11-12T10:43:30.711627Z","shell.execute_reply":"2024-11-12T10:43:30.716577Z"},"trusted":true},"outputs":[],"execution_count":103},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\nimport os\nimport glob\nimport math\n\ndef plot_yolo_predictions_and_ground_truth(image_dir, label_dir, model, pattern=\"*\", label_paths=[], results = None, **kwargs):\n    \"\"\"\n    Plot YOLO predictions and ground truth labels on images.\n\n    Parameters:\n    - image_dir (str): Directory containing the images.\n    - label_dir (str): Directory containing the label files.\n    - model: YOLO model for making predictions.\n    - pattern (str): Pattern to match image files in the directory.\n    \"\"\"\n    # Get list of images\n    image_paths = sorted(glob.glob(os.path.join(image_dir, pattern)),key=extract_numbers)\n    \n    # Run batch inference\n    results = model(image_paths, **kwargs)\n\n    # Plot image with ground truth and predicted boxes\n    num_rows = math.ceil(len(image_paths)/4)\n    fig, ax = plt.subplots(num_rows, 4, figsize=(16, 16))      \n    \n    if num_rows > 1:\n        ax = ax.flatten()\n    else:\n        ax = [ax]  # Make it iterable for consistency    \n    \n    imgs = []\n    for i, image_path in enumerate(image_paths):\n        # Load image\n        img = cv2.imread(image_path)\n        imgs.append(img)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        gt_boxes = []        \n        if label_dir or label_paths:\n            # Get corresponding label file\n            if label_paths:\n                label_path = label_paths[image_path]\n            else:\n                label_path = os.path.join(label_dir, os.path.basename(image_path).replace('.png', '.txt'))\n\n            if os.path.exists(label_path):\n                # Read ground truth labels\n                with open(label_path, 'r') as f:\n                    gt_labels = f.read()\n\n                gt_labels = clean_newlines(gt_labels)\n\n                # Convert ground truth labels to bounding boxes\n                if gt_labels:\n                    for label in gt_labels:\n                        cls, x, y, w, h = map(float, label.strip().split())\n                        x_min = int((x - w / 2) * img.shape[1])\n                        y_min = int((y - h / 2) * img.shape[0])\n                        x_max = int((x + w / 2) * img.shape[1])\n                        y_max = int((y + h / 2) * img.shape[0])\n                        gt_boxes.append([x_min, y_min, x_max, y_max, int(cls)])\n\n        # Get prediction results for the current image\n        result = results[i]\n        pred_boxes = result.boxes.xyxy.cpu().numpy()  # Convert to numpy array\n        pred_cls = result.boxes.cls.cpu().numpy()\n        pred_conf = result.boxes.conf.cpu().numpy()\n        \n        ax[i].imshow(img_rgb)\n        \n        # Plot ground truth boxes\n        for box in gt_boxes:\n            x_min, y_min, x_max, y_max, cls = box\n            rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, fill=False, color='green', linewidth=2)\n            ax[i].add_patch(rect)\n            ax[i].text(x_min, y_min - 2, f'GT {cls}', fontsize=12, color='green')\n        \n        # Plot predicted boxes\n        for box, cls, conf in zip(pred_boxes, pred_cls, pred_conf):\n            x_min, y_min, x_max, y_max = box\n            rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, fill=False, color='red', linewidth=2)\n            ax[i].add_patch(rect)\n            ax[i].text(x_min, y_min - 2, f'Pred {cls:.0f} ({conf:.2f})', fontsize=12, color='red')\n        \n    # Show the result\n    plt.show()\n        \n    return results, imgs\n        \n# Example usage:\n# model = ...  # Load your YOLO model here\n# plot_yolo_predictions_and_ground_truth(\n#     image_dir=\"/kaggle/input/rsna-2024-yolo/yolo_zip/images/val\",\n#     label_dir=\"/kaggle/input/rsna-2024-yolo/yolo_zip/labels/val\",\n#     model=model,\n#     pattern=\"1018005303*\"\n# )\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:43:30.718854Z","iopub.execute_input":"2024-11-12T10:43:30.719109Z","iopub.status.idle":"2024-11-12T10:43:30.743385Z","shell.execute_reply.started":"2024-11-12T10:43:30.719086Z","shell.execute_reply":"2024-11-12T10:43:30.742497Z"},"trusted":true},"outputs":[],"execution_count":104},{"cell_type":"code","source":"STUDY_ID = \"1008446160_3775545364\"\nimage_dir=\"/kaggle/working/yolo_set/images/inference/\"\npattern=f\"{STUDY_ID}*\"\nos.path.join(image_dir, pattern)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:43:30.745229Z","iopub.execute_input":"2024-11-12T10:43:30.745529Z","iopub.status.idle":"2024-11-12T10:43:30.757334Z","shell.execute_reply.started":"2024-11-12T10:43:30.745495Z","shell.execute_reply":"2024-11-12T10:43:30.756434Z"},"trusted":true},"outputs":[{"execution_count":105,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/yolo_set/images/inference/1008446160_3775545364*'"},"metadata":{}}],"execution_count":105},{"cell_type":"code","source":"study_id_meta[4646740]","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:43:35.303682Z","iopub.execute_input":"2024-11-12T10:43:35.304102Z","iopub.status.idle":"2024-11-12T10:43:35.311383Z","shell.execute_reply.started":"2024-11-12T10:43:35.304068Z","shell.execute_reply":"2024-11-12T10:43:35.310411Z"},"trusted":true},"outputs":[{"execution_count":106,"output_type":"execute_result","data":{"text/plain":"{'folder_path': '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/4646740',\n 'series_id_files': [3201256954],\n 'series_descriptions': ['Axial T2']}"},"metadata":{}}],"execution_count":106},{"cell_type":"code","source":"image_paths = sorted(glob.glob(os.path.join(image_dir, pattern)),key=extract_numbers)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:43:36.283559Z","iopub.execute_input":"2024-11-12T10:43:36.284275Z","iopub.status.idle":"2024-11-12T10:43:36.404291Z","shell.execute_reply.started":"2024-11-12T10:43:36.284227Z","shell.execute_reply":"2024-11-12T10:43:36.403346Z"},"trusted":true},"outputs":[],"execution_count":107},{"cell_type":"code","source":"MODEL_DIR = \"/kaggle/input/rsna-yolo-v1/pytorch/best_20240716/2/best_20240716.pt\" if DATASET_SELECTION=='sagittal' else \"/kaggle/input/rsna-yolo-v1/pytorch/best_20240716/4/best_axial_20241106.pt\"\n# MODEL_DIR='/kaggle/input/rsna-yolo-v1/pytorch/best_20240716/2/best_sagittal_20241105.pt'\n\nif SUBMISSION:\n    model = YOLO(MODEL_DIR)\n    STUDY_ID = \"1777645381_1438513204\" # 1018005303 1019430579 105895264 \n\n    results, imgs = plot_yolo_predictions_and_ground_truth(\n        image_dir=\"/kaggle/working/yolo_set/images/inference/\",\n        label_dir=None,\n        model=model,\n        pattern=f\"{STUDY_ID}*\",\n        conf=0.25,\n        iou=0.8\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SUBMISSION","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:43:38.198706Z","iopub.status.idle":"2024-11-12T10:43:38.199075Z","shell.execute_reply.started":"2024-11-12T10:43:38.198895Z","shell.execute_reply":"2024-11-12T10:43:38.198910Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2D Boxes into 3D segments - an algorithm?\n\nPreprocessing:\n- Take the highest probability box, for each class per instance.\n- [DONE via YOLO?] If there are any boxes with IoU of more than 0.8 take the highest probability, this is regardless of class, as we know the boundaries between different segments are distinct. YOLO might do this per class, but not independent of class.\n\n**At this stage we assume only for Sagittal**, when other MRI types are incorporate, we'll have a list of study_ids for each MRI type (and they'll be indexed by studyid_seriesid.\n\nApproach:\n- Take each 2D slice, grab ROI points, and fill in the following dictionary information.\n    ```python\n    {\n    study_id:\n        {\n        class_0: {instance_no: [], box_coord: [], box_conf: [], left_right: []}\n        }\n    }\n\n    ```\n- Perform calculation to grab 3D segments:\n    - If there is a gap of more than 1 instance, then discard outer segment. TODO decide on approach for left/right neuronamial farrowing.\n    - Calculate the maximum region across each slice, by adjusting for the positioning of the  x,y coords stacking images, and then taking the maximum area. For example, we essentially want to obtain adjusted, max_x, max_y, min_x, min_y, and that becomes the new bounding box, for the voxel.\n    - [POSSIBLE] Grab centre coordinate and discard those where they are > % away from standard distribution of centre coords in dataset","metadata":{}},{"cell_type":"code","source":"# TODO - add in series_ids\n# Set up struture for box metadata\nboxes_meta = {}\nclass_list = [0,1,2,3,4] if DATASET_SELECTION == 'sagittal' else [0,1]\nfor key, values in study_id_meta.items():\n    for series in values['series_id_files']:\n        boxes_meta.update({f\"{key}_{series}\": {i:{'instance_no':[],\"box_coord\":[],\"box_conf\":[], \"left_right\":[]} for i in class_list}})\n# slice_result = results[4]","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:43:45.132675Z","iopub.execute_input":"2024-11-12T10:43:45.133727Z","iopub.status.idle":"2024-11-12T10:43:45.203417Z","shell.execute_reply.started":"2024-11-12T10:43:45.133683Z","shell.execute_reply":"2024-11-12T10:43:45.202421Z"},"trusted":true},"outputs":[],"execution_count":109},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:43:45.301672Z","iopub.execute_input":"2024-11-12T10:43:45.302015Z","iopub.status.idle":"2024-11-12T10:43:45.352135Z","shell.execute_reply.started":"2024-11-12T10:43:45.301985Z","shell.execute_reply":"2024-11-12T10:43:45.351071Z"},"trusted":true},"outputs":[],"execution_count":110},{"cell_type":"code","source":"import gc\ngc.collect()  # Force garbage collection\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:43:45.454619Z","iopub.execute_input":"2024-11-12T10:43:45.455670Z","iopub.status.idle":"2024-11-12T10:43:45.842225Z","shell.execute_reply.started":"2024-11-12T10:43:45.455628Z","shell.execute_reply":"2024-11-12T10:43:45.841048Z"},"trusted":true},"outputs":[{"execution_count":111,"output_type":"execute_result","data":{"text/plain":"4727"},"metadata":{}}],"execution_count":111},{"cell_type":"code","source":"INFERENCE_BATCH_SIZE = 100\nno_batches = (len(files_for_inference)//INFERENCE_BATCH_SIZE)+1\nbatches_files_for_inference = [files_for_inference[((i*INFERENCE_BATCH_SIZE)):(i+1)*INFERENCE_BATCH_SIZE if (i+1) < no_batches else len(files_for_inference)] for i in range(no_batches)]\nassert batches_files_for_inference[no_batches-1][-1] == files_for_inference[-1]","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:43:45.844328Z","iopub.execute_input":"2024-11-12T10:43:45.844669Z","iopub.status.idle":"2024-11-12T10:43:45.860852Z","shell.execute_reply.started":"2024-11-12T10:43:45.844638Z","shell.execute_reply":"2024-11-12T10:43:45.859932Z"},"trusted":true},"outputs":[],"execution_count":112},{"cell_type":"code","source":"batches_files_for_inference[no_batches-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T10:43:45.862072Z","iopub.execute_input":"2024-11-12T10:43:45.862561Z","iopub.status.idle":"2024-11-12T10:43:45.874593Z","shell.execute_reply.started":"2024-11-12T10:43:45.862530Z","shell.execute_reply":"2024-11-12T10:43:45.873654Z"}},"outputs":[{"execution_count":113,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/yolo_set/images/inference/991428866_3160509931_26.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_27.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_28.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_29.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_3.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_30.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_31.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_32.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_33.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_34.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_35.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_36.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_37.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_38.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_39.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_4.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_40.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_41.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_42.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_43.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_44.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_45.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_46.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_47.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_48.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_49.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_5.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_50.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_51.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_52.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_53.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_54.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_6.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_7.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_8.png',\n '/kaggle/working/yolo_set/images/inference/991428866_3160509931_9.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_1.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_10.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_11.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_12.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_13.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_14.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_15.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_16.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_17.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_18.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_19.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_2.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_20.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_21.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_22.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_3.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_4.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_5.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_6.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_7.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_8.png',\n '/kaggle/working/yolo_set/images/inference/992674144_1614310972_9.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_1.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_10.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_11.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_12.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_13.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_14.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_15.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_16.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_17.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_18.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_19.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_2.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_20.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_21.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_3.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_4.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_5.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_6.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_7.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_8.png',\n '/kaggle/working/yolo_set/images/inference/998688940_2089880748_9.png']"},"metadata":{}}],"execution_count":113},{"cell_type":"markdown","source":"### Getting around YOLO\n\nYOLO doesn't seem to use GPU memory properly, `stream=True` doesn't decrease GPU memory utlisation enough. So performing one's own batching.","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:43:47.252134Z","iopub.execute_input":"2024-11-12T10:43:47.252536Z","iopub.status.idle":"2024-11-12T10:43:47.260541Z","shell.execute_reply.started":"2024-11-12T10:43:47.252498Z","shell.execute_reply":"2024-11-12T10:43:47.259567Z"},"trusted":true},"outputs":[],"execution_count":114},{"cell_type":"code","source":"for ndx, files in enumerate(batches_files_for_inference):\n    \n    results = model(files,task='detect',iou=0.8,stream=True, cache=False, batch=1, save=False, verbose=False)  # return a list of Results objects\n    \n    for slice_result in results:\n        if slice_result.boxes:\n            img_path = slice_result.path\n            study_id, series_id, instance_no = extract_numbers(img_path)\n            box = slice_result.boxes\n            classes = box.cls.cpu()\n            conf = box.conf.cpu()\n            coord = box.xywh.cpu()\n            id_key = f\"{study_id}_{series_id}\"\n            for i, class_ in enumerate(classes):\n                class_ = int(class_)\n                boxes_meta[id_key][class_]['instance_no'].append(instance_no)\n                boxes_meta[id_key][class_]['box_conf'].append(conf[i])\n                boxes_meta[id_key][class_]['box_coord'].append(coord[i])\n    \n    if ndx%10==0:\n        print(f\"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, Processed {ndx*INFERENCE_BATCH_SIZE} images.\")\n                ","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:43:48.703043Z","iopub.execute_input":"2024-11-12T10:43:48.703912Z","iopub.status.idle":"2024-11-12T10:56:28.420256Z","shell.execute_reply.started":"2024-11-12T10:43:48.703880Z","shell.execute_reply":"2024-11-12T10:56:28.419397Z"},"trusted":true},"outputs":[{"name":"stdout","text":"2024-11-12 10:43:49, Processed 0 images.\n2024-11-12 10:43:59, Processed 1000 images.\n2024-11-12 10:44:09, Processed 2000 images.\n2024-11-12 10:44:18, Processed 3000 images.\n2024-11-12 10:44:28, Processed 4000 images.\n2024-11-12 10:44:38, Processed 5000 images.\n2024-11-12 10:44:47, Processed 6000 images.\n2024-11-12 10:44:57, Processed 7000 images.\n2024-11-12 10:45:06, Processed 8000 images.\n2024-11-12 10:45:16, Processed 9000 images.\n2024-11-12 10:45:25, Processed 10000 images.\n2024-11-12 10:45:35, Processed 11000 images.\n2024-11-12 10:45:44, Processed 12000 images.\n2024-11-12 10:45:54, Processed 13000 images.\n2024-11-12 10:46:04, Processed 14000 images.\n2024-11-12 10:46:13, Processed 15000 images.\n2024-11-12 10:46:23, Processed 16000 images.\n2024-11-12 10:46:32, Processed 17000 images.\n2024-11-12 10:46:42, Processed 18000 images.\n2024-11-12 10:46:51, Processed 19000 images.\n2024-11-12 10:47:00, Processed 20000 images.\n2024-11-12 10:47:10, Processed 21000 images.\n2024-11-12 10:47:19, Processed 22000 images.\n2024-11-12 10:47:29, Processed 23000 images.\n2024-11-12 10:47:38, Processed 24000 images.\n2024-11-12 10:47:48, Processed 25000 images.\n2024-11-12 10:47:57, Processed 26000 images.\n2024-11-12 10:48:07, Processed 27000 images.\n2024-11-12 10:48:16, Processed 28000 images.\n2024-11-12 10:48:26, Processed 29000 images.\n2024-11-12 10:48:35, Processed 30000 images.\n2024-11-12 10:48:45, Processed 31000 images.\n2024-11-12 10:48:54, Processed 32000 images.\n2024-11-12 10:49:04, Processed 33000 images.\n2024-11-12 10:49:13, Processed 34000 images.\n2024-11-12 10:49:23, Processed 35000 images.\n2024-11-12 10:49:32, Processed 36000 images.\n2024-11-12 10:49:42, Processed 37000 images.\n2024-11-12 10:49:51, Processed 38000 images.\n2024-11-12 10:50:01, Processed 39000 images.\n2024-11-12 10:50:10, Processed 40000 images.\n2024-11-12 10:50:20, Processed 41000 images.\n2024-11-12 10:50:29, Processed 42000 images.\n2024-11-12 10:50:39, Processed 43000 images.\n2024-11-12 10:50:48, Processed 44000 images.\n2024-11-12 10:50:58, Processed 45000 images.\n2024-11-12 10:51:07, Processed 46000 images.\n2024-11-12 10:51:17, Processed 47000 images.\n2024-11-12 10:51:26, Processed 48000 images.\n2024-11-12 10:51:35, Processed 49000 images.\n2024-11-12 10:51:45, Processed 50000 images.\n2024-11-12 10:51:54, Processed 51000 images.\n2024-11-12 10:52:04, Processed 52000 images.\n2024-11-12 10:52:13, Processed 53000 images.\n2024-11-12 10:52:23, Processed 54000 images.\n2024-11-12 10:52:32, Processed 55000 images.\n2024-11-12 10:52:42, Processed 56000 images.\n2024-11-12 10:52:51, Processed 57000 images.\n2024-11-12 10:53:01, Processed 58000 images.\n2024-11-12 10:53:10, Processed 59000 images.\n2024-11-12 10:53:20, Processed 60000 images.\n2024-11-12 10:53:29, Processed 61000 images.\n2024-11-12 10:53:39, Processed 62000 images.\n2024-11-12 10:53:48, Processed 63000 images.\n2024-11-12 10:53:58, Processed 64000 images.\n2024-11-12 10:54:07, Processed 65000 images.\n2024-11-12 10:54:17, Processed 66000 images.\n2024-11-12 10:54:26, Processed 67000 images.\n2024-11-12 10:54:36, Processed 68000 images.\n2024-11-12 10:54:45, Processed 69000 images.\n2024-11-12 10:54:55, Processed 70000 images.\n2024-11-12 10:55:04, Processed 71000 images.\n2024-11-12 10:55:13, Processed 72000 images.\n2024-11-12 10:55:23, Processed 73000 images.\n2024-11-12 10:55:32, Processed 74000 images.\n2024-11-12 10:55:42, Processed 75000 images.\n2024-11-12 10:55:51, Processed 76000 images.\n2024-11-12 10:56:01, Processed 77000 images.\n2024-11-12 10:56:10, Processed 78000 images.\n2024-11-12 10:56:20, Processed 79000 images.\n","output_type":"stream"}],"execution_count":115},{"cell_type":"code","source":"import pickle\nwith open(f'/kaggle/working/{SET_TYPE}_{DATASET_SELECTION}_boxes_meta.pkl', 'wb') as file:\n    pickle.dump(boxes_meta, file)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:56:28.421823Z","iopub.execute_input":"2024-11-12T10:56:28.422090Z","iopub.status.idle":"2024-11-12T10:56:30.150911Z","shell.execute_reply.started":"2024-11-12T10:56:28.422068Z","shell.execute_reply":"2024-11-12T10:56:30.150110Z"},"trusted":true},"outputs":[],"execution_count":116},{"cell_type":"code","source":"import pickle\nboxes_meta = pickle.load(open(f\"/kaggle/input/rsna-2024-train-candidate-list/{SET_TYPE}_{DATASET_SELECTION}_boxes_meta.pkl\",\"rb\"))\n\nboxes_meta = pickle.load(open(f\"/kaggle/input/rsna-2024-train-candidate-list/train_sagittal_boxes_meta.pkl\",\"rb\"))","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:01:42.027084Z","iopub.execute_input":"2024-11-12T10:01:42.027519Z","iopub.status.idle":"2024-11-12T10:01:55.254424Z","shell.execute_reply.started":"2024-11-12T10:01:42.027485Z","shell.execute_reply":"2024-11-12T10:01:55.253550Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"study_id_meta[4003253]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T10:05:26.119164Z","iopub.execute_input":"2024-11-12T10:05:26.119847Z","iopub.status.idle":"2024-11-12T10:05:26.125766Z","shell.execute_reply.started":"2024-11-12T10:05:26.119813Z","shell.execute_reply":"2024-11-12T10:05:26.124919Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"{'folder_path': '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/4003253',\n 'series_id_files': [702807833, 1054713880],\n 'series_descriptions': ['Sagittal T2/STIR', 'Sagittal T1']}"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"boxes_meta[\"4003253_1054713880\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"boxes_meta[list(boxes_meta.keys())[0]], list(boxes_meta.keys())[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Get Image MetaData**\n\nGet dicom image meta data for all Sagittal T1 images, such that we can attribute boxes to L/R estimate.","metadata":{}},{"cell_type":"code","source":"sagittal_t1_series_ids = [\n    f\"{key}_{series_id}\"\n    for key, entry in study_id_meta.items()\n    for series_id, description in zip(entry['series_id_files'], entry['series_descriptions'])\n    if description == 'Sagittal T1'\n]","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:07:00.695632Z","iopub.execute_input":"2024-11-12T10:07:00.696028Z","iopub.status.idle":"2024-11-12T10:07:00.704201Z","shell.execute_reply.started":"2024-11-12T10:07:00.695995Z","shell.execute_reply":"2024-11-12T10:07:00.703228Z"},"trusted":true},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# TODO DELETE\n# Get all image_list, then filter for Sagittal T1\nDICOM_DATA_PATH = f\"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/{SET_TYPE}_images/\" \nimages = glob.glob(os.path.join(DICOM_DATA_PATH,\"*/*/*.dcm\"), recursive=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:07:01.074856Z","iopub.execute_input":"2024-11-12T10:07:01.075693Z","iopub.status.idle":"2024-11-12T10:07:08.227012Z","shell.execute_reply.started":"2024-11-12T10:07:01.075656Z","shell.execute_reply":"2024-11-12T10:07:08.225911Z"},"trusted":true},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def rescale_bbox(bbox_xyxy: np.array, original_w_h=()):    \n    # Original image size\n    original_width = original_w_h[0]\n    original_height = original_w_h[1]\n\n    # Resized image size\n    resized_width = IMG_RESIZE[0]\n    resized_height = IMG_RESIZE[1]\n\n    # Scaling factors\n    scale_x = original_width / resized_width\n    scale_y = original_height / resized_height\n\n    # Upscale the bounding box coordinates to the original image size\n    bbox_original = bbox_xyxy * np.array([scale_x, scale_y, scale_x, scale_y])\n\n    # bbox_original now contains the upscaled coordinates\n    x1_upscaled, y1_upscaled, x2_upscaled, y2_upscaled = bbox_original\n    \n    return (int(x1_upscaled), int(y1_upscaled), int(x2_upscaled), int(y2_upscaled))","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:07:08.228968Z","iopub.execute_input":"2024-11-12T10:07:08.229444Z","iopub.status.idle":"2024-11-12T10:07:08.236034Z","shell.execute_reply.started":"2024-11-12T10:07:08.229411Z","shell.execute_reply":"2024-11-12T10:07:08.235148Z"},"trusted":true},"outputs":[],"execution_count":50},{"cell_type":"code","source":"from torch import tensor\ndef get_xyxy(boxes: list, return_int=True) -> tuple:\n    \"\"\"\n    Get largest region in xyxy coordinates given a set of multiple slices of xywh coordinates.\n    \"\"\"\n    if isinstance(boxes,list):\n        boxes = torch.stack(boxes)\n    elif isinstance(boxes, torch.Tensor) and len(boxes.shape)==1:\n        boxes = boxes.unsqueeze(0)\n\n    x_centers = boxes[:, 0]\n    y_centers = boxes[:, 1]\n    widths = boxes[:, 2]\n    heights = boxes[:, 3]\n\n    x_mins = x_centers - widths / 2\n    y_mins = y_centers - heights / 2\n    x_maxs = x_centers + widths / 2\n    y_maxs = y_centers + heights / 2\n\n    # Compute the desired max and min values\n    min_x = torch.min(x_mins).item()\n    max_x = torch.max(x_maxs).item()\n    min_y = torch.min(y_mins).item()\n    max_y = torch.max(y_maxs).item()\n\n    if return_int:\n        min_x = int(min_x)\n        max_x = int(max_x)\n        min_y = int(min_y)\n        max_y = int(max_y)\n    \n    return (min_x, max_x, min_y, max_y)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:07:08.237284Z","iopub.execute_input":"2024-11-12T10:07:08.237569Z","iopub.status.idle":"2024-11-12T10:07:08.252283Z","shell.execute_reply.started":"2024-11-12T10:07:08.237537Z","shell.execute_reply":"2024-11-12T10:07:08.251400Z"},"trusted":true},"outputs":[],"execution_count":51},{"cell_type":"code","source":"def get_box_stats(data: dict):\n    if data['instance_no']:\n        data['roi_xyxy'] = get_xyxy(data['box_coord'])\n        max_conf = torch.argmax(torch.tensor(data['box_conf'])).item() \n        data['conf_xyxy'] = get_xyxy(data['box_coord'][max_conf])\n        data['conf_instance_no'] = data['instance_no'][max_conf]\n    else:\n        data['roi_xyxy'] = ()\n        data['conf_xyxy'] = ()\n        data['conf_instance_no'] = np.NaN\n        \n        \n    ","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:07:08.254262Z","iopub.execute_input":"2024-11-12T10:07:08.254671Z","iopub.status.idle":"2024-11-12T10:07:08.266969Z","shell.execute_reply.started":"2024-11-12T10:07:08.254640Z","shell.execute_reply":"2024-11-12T10:07:08.266105Z"},"trusted":true},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":"Obtain `new_data`, a bounding box region for each condition and class.\n\nTo obtain left/right for neural foraminal narrowing we see use the `instance_number` of dicom meta data. Images are taken right to left, the assumption being images on the right (left) hand side will be below (above) the median/average. This crude method results in 99.4% accuracy on train set.\n\n| left_or_right_class | left_or_right_avg | Count |\n|---------------------|-------------------|-------|\n| left                | left              | 9790  |\n|                     | right             | 70    |\n| right               | left              | 41    |\n|                     | right             | 9788  |\n","metadata":{}},{"cell_type":"markdown","source":"Reading in descriptive train.csv with all study information.","metadata":{}},{"cell_type":"code","source":"train_desc = datasets['sagittal'].copy()\n\ntrain_desc.study_id = train_desc.study_id.astype(str)\ntrain_desc.series_id = train_desc.series_id.astype(str)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:07:21.610463Z","iopub.execute_input":"2024-11-12T10:07:21.611146Z","iopub.status.idle":"2024-11-12T10:07:21.621211Z","shell.execute_reply.started":"2024-11-12T10:07:21.611114Z","shell.execute_reply":"2024-11-12T10:07:21.620359Z"},"trusted":true},"outputs":[],"execution_count":53},{"cell_type":"code","source":"x = pd.DataFrame.from_dict(boxes_meta,orient='index')\n\nfor col in x.columns:\n    x[f'{col}_populated'] = x.apply(lambda x: True if x[col]['instance_no'] else False, axis=1)\n\n# Get study and series id from index\nx = x.reset_index()\nx.rename(columns={'index': 'study_series'}, inplace=True)\nx[['study_id', 'series_id']] = x['study_series'].str.split('_', expand=True)\n\n\nx.dtypes, train_desc.dtypes\n\nx.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:07:23.871989Z","iopub.execute_input":"2024-11-12T10:07:23.872836Z","iopub.status.idle":"2024-11-12T10:07:24.313869Z","shell.execute_reply.started":"2024-11-12T10:07:23.872803Z","shell.execute_reply":"2024-11-12T10:07:24.312967Z"},"trusted":true},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"         study_series                                                  0  \\\n0   4003253_702807833  {'instance_no': [11, 6], 'box_coord': [[tensor...   \n1  4003253_1054713880  {'instance_no': [11, 6], 'box_coord': [[tensor...   \n2  4646740_3486248476  {'instance_no': [16, 17, 6, 7, 8], 'box_coord'...   \n3  4646740_3666319702  {'instance_no': [16, 5], 'box_coord': [[tensor...   \n4   7143189_132939515  {'instance_no': [], 'box_coord': [], 'box_conf...   \n\n                                                   1  \\\n0  {'instance_no': [12, 12, 5, 6], 'box_coord': [...   \n1  {'instance_no': [11, 12, 5, 6], 'box_coord': [...   \n2  {'instance_no': [15, 16, 5, 6], 'box_coord': [...   \n3  {'instance_no': [], 'box_coord': [], 'box_conf...   \n4  {'instance_no': [], 'box_coord': [], 'box_conf...   \n\n                                                   2  \\\n0  {'instance_no': [12, 5], 'box_coord': [[tensor...   \n1  {'instance_no': [11, 12, 5], 'box_coord': [[te...   \n2  {'instance_no': [15, 4, 5], 'box_coord': [[ten...   \n3  {'instance_no': [], 'box_coord': [], 'box_conf...   \n4  {'instance_no': [], 'box_coord': [], 'box_conf...   \n\n                                                   3  \\\n0  {'instance_no': [11, 4], 'box_coord': [[tensor...   \n1  {'instance_no': [11, 12, 4], 'box_coord': [[te...   \n2  {'instance_no': [16, 17, 3, 4, 5], 'box_coord'...   \n3  {'instance_no': [17], 'box_coord': [[tensor(25...   \n4  {'instance_no': [], 'box_coord': [], 'box_conf...   \n\n                                                   4  0_populated  \\\n0  {'instance_no': [11, 3, 4, 4], 'box_coord': [[...         True   \n1  {'instance_no': [11, 12, 3, 4], 'box_coord': [...         True   \n2  {'instance_no': [5, 6, 7], 'box_coord': [[tens...         True   \n3  {'instance_no': [], 'box_coord': [], 'box_conf...         True   \n4  {'instance_no': [], 'box_coord': [], 'box_conf...        False   \n\n   1_populated  2_populated  3_populated  4_populated study_id   series_id  \n0         True         True         True         True  4003253   702807833  \n1         True         True         True         True  4003253  1054713880  \n2         True         True         True         True  4646740  3486248476  \n3        False        False         True        False  4646740  3666319702  \n4        False        False        False        False  7143189   132939515  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>study_series</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>0_populated</th>\n      <th>1_populated</th>\n      <th>2_populated</th>\n      <th>3_populated</th>\n      <th>4_populated</th>\n      <th>study_id</th>\n      <th>series_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4003253_702807833</td>\n      <td>{'instance_no': [11, 6], 'box_coord': [[tensor...</td>\n      <td>{'instance_no': [12, 12, 5, 6], 'box_coord': [...</td>\n      <td>{'instance_no': [12, 5], 'box_coord': [[tensor...</td>\n      <td>{'instance_no': [11, 4], 'box_coord': [[tensor...</td>\n      <td>{'instance_no': [11, 3, 4, 4], 'box_coord': [[...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>4003253</td>\n      <td>702807833</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4003253_1054713880</td>\n      <td>{'instance_no': [11, 6], 'box_coord': [[tensor...</td>\n      <td>{'instance_no': [11, 12, 5, 6], 'box_coord': [...</td>\n      <td>{'instance_no': [11, 12, 5], 'box_coord': [[te...</td>\n      <td>{'instance_no': [11, 12, 4], 'box_coord': [[te...</td>\n      <td>{'instance_no': [11, 12, 3, 4], 'box_coord': [...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>4003253</td>\n      <td>1054713880</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4646740_3486248476</td>\n      <td>{'instance_no': [16, 17, 6, 7, 8], 'box_coord'...</td>\n      <td>{'instance_no': [15, 16, 5, 6], 'box_coord': [...</td>\n      <td>{'instance_no': [15, 4, 5], 'box_coord': [[ten...</td>\n      <td>{'instance_no': [16, 17, 3, 4, 5], 'box_coord'...</td>\n      <td>{'instance_no': [5, 6, 7], 'box_coord': [[tens...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>4646740</td>\n      <td>3486248476</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4646740_3666319702</td>\n      <td>{'instance_no': [16, 5], 'box_coord': [[tensor...</td>\n      <td>{'instance_no': [], 'box_coord': [], 'box_conf...</td>\n      <td>{'instance_no': [], 'box_coord': [], 'box_conf...</td>\n      <td>{'instance_no': [17], 'box_coord': [[tensor(25...</td>\n      <td>{'instance_no': [], 'box_coord': [], 'box_conf...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>4646740</td>\n      <td>3666319702</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7143189_132939515</td>\n      <td>{'instance_no': [], 'box_coord': [], 'box_conf...</td>\n      <td>{'instance_no': [], 'box_coord': [], 'box_conf...</td>\n      <td>{'instance_no': [], 'box_coord': [], 'box_conf...</td>\n      <td>{'instance_no': [], 'box_coord': [], 'box_conf...</td>\n      <td>{'instance_no': [], 'box_coord': [], 'box_conf...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>7143189</td>\n      <td>132939515</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"train_pred = train_desc.merge(right=x, how='left',on=['study_id','series_id'])","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:07:39.800238Z","iopub.execute_input":"2024-11-12T10:07:39.800624Z","iopub.status.idle":"2024-11-12T10:07:39.821174Z","shell.execute_reply.started":"2024-11-12T10:07:39.800591Z","shell.execute_reply":"2024-11-12T10:07:39.820307Z"},"trusted":true},"outputs":[],"execution_count":55},{"cell_type":"code","source":"assert any(train_pred.study_series.isna())==False","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:08:25.154953Z","iopub.execute_input":"2024-11-12T10:08:25.155335Z","iopub.status.idle":"2024-11-12T10:08:25.161508Z","shell.execute_reply.started":"2024-11-12T10:08:25.155302Z","shell.execute_reply":"2024-11-12T10:08:25.160589Z"},"trusted":true},"outputs":[],"execution_count":61},{"cell_type":"code","source":"pop_cols = [\"0_populated\",\"1_populated\",\"2_populated\",\"3_populated\",\"4_populated\"]\nfor pop_col in pop_cols:\n    print(train_pred.groupby(['series_description', pop_col]).size())","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:08:37.015153Z","iopub.execute_input":"2024-11-12T10:08:37.015540Z","iopub.status.idle":"2024-11-12T10:08:37.035697Z","shell.execute_reply.started":"2024-11-12T10:08:37.015509Z","shell.execute_reply":"2024-11-12T10:08:37.034744Z"},"trusted":true},"outputs":[{"name":"stdout","text":"series_description  0_populated\nSagittal T1         False             2\n                    True           1978\nSagittal T2/STIR    False           387\n                    True           1587\ndtype: int64\nseries_description  1_populated\nSagittal T1         False             8\n                    True           1972\nSagittal T2/STIR    False           505\n                    True           1469\ndtype: int64\nseries_description  2_populated\nSagittal T1         False             7\n                    True           1973\nSagittal T2/STIR    False           521\n                    True           1453\ndtype: int64\nseries_description  3_populated\nSagittal T1         False             8\n                    True           1972\nSagittal T2/STIR    False           554\n                    True           1420\ndtype: int64\nseries_description  4_populated\nSagittal T1         False             3\n                    True           1977\nSagittal T2/STIR    False           368\n                    True           1606\ndtype: int64\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"x.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-11-03T04:07:49.483547Z","iopub.execute_input":"2024-11-03T04:07:49.483891Z","iopub.status.idle":"2024-11-03T04:07:49.491444Z","shell.execute_reply.started":"2024-11-03T04:07:49.483866Z","shell.execute_reply":"2024-11-03T04:07:49.490530Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pop_cols = ['0_populated', '1_populated', '2_populated', '3_populated', '4_populated']","metadata":{"execution":{"iopub.status.busy":"2024-11-03T04:30:05.202979Z","iopub.execute_input":"2024-11-03T04:30:05.203830Z","iopub.status.idle":"2024-11-03T04:30:05.207983Z","shell.execute_reply.started":"2024-11-03T04:30:05.203798Z","shell.execute_reply":"2024-11-03T04:30:05.206969Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Analysing the predictions from YOLO","metadata":{}},{"cell_type":"code","source":"for col in pop_cols:\n    print(train_pred.groupby(col).size())","metadata":{"execution":{"iopub.status.busy":"2024-11-03T04:07:29.245699Z","iopub.execute_input":"2024-11-03T04:07:29.246057Z","iopub.status.idle":"2024-11-03T04:07:29.274278Z","shell.execute_reply.started":"2024-11-03T04:07:29.246029Z","shell.execute_reply":"2024-11-03T04:07:29.273306Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO two approaches (1) use simple look up against metadata_df, for the instance number on whether left/right, (2) calculate the averages within the box (which \n# heavily relies on both right/left predictions existing)\n\n# Approach (2) here\nnew_data = {}\nfor study_series, items in boxes_meta.items():\n    preds = items\n#     for preds in items.values(): # Not at series level yet\n\n    # Separate classes into left/right based on position of instance number\n    for level in class_list:\n        \n        level_class = LUMBAR_INT_MAPPING[level]\n                \n        if study_series not in sagittal_t1_series_ids:\n            inner_value = preds[level]\n            get_box_stats(inner_value)\n            \n            new_data[f'{study_series}_spinal_canal_stenosis_{level_class}'] = inner_value            \n            \n            \n        else:\n        \n            if level in preds.keys() and preds[level]['instance_no']:\n                instance_nos = preds[level]['instance_no']\n                try:\n                    avg_in = np.mean(instance_nos)\n                except Exception:\n                    print(instance_nos)\n                preds[level]['left_right'] = [\"left\" if i > avg_in else \"right\" for i in instance_nos]\n\n            inner_value = preds[level]\n            left_data = {'instance_no': [], 'box_coord': [], 'box_conf': [], 'roi_xyxy': ()}\n            right_data = {'instance_no': [], 'box_coord': [], 'box_conf': [], 'roi_xyxy': ()}\n\n            for i, lr in enumerate(inner_value['left_right']):\n                if lr == 'left':\n                    left_data['instance_no'].append(inner_value['instance_no'][i])\n                    left_data['box_coord'].append(inner_value['box_coord'][i])\n                    left_data['box_conf'].append(inner_value['box_conf'][i])\n                elif lr == 'right':\n                    right_data['instance_no'].append(inner_value['instance_no'][i])\n                    right_data['box_coord'].append(inner_value['box_coord'][i])\n                    right_data['box_conf'].append(inner_value['box_conf'][i])\n\n            if right_data['box_coord']:\n                get_box_stats(right_data)\n\n            if left_data['box_coord']:\n                get_box_stats(left_data)\n\n            if left_data['instance_no']:\n                new_data[f'{study_series}_left_neural_foraminal_narrowing_{level_class}'] = left_data\n            if right_data['instance_no']:\n                new_data[f'{study_series}_right_neural_foraminal_narrowing_{level_class}'] = right_data","metadata":{"execution":{"iopub.status.busy":"2024-11-03T04:37:38.084097Z","iopub.execute_input":"2024-11-03T04:37:38.085244Z","iopub.status.idle":"2024-11-03T04:37:44.692147Z","shell.execute_reply.started":"2024-11-03T04:37:38.085202Z","shell.execute_reply":"2024-11-03T04:37:44.691238Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for key, value in new_data.items():\n    study_id, series_id = tuple(key.split(\"_\")[0:2])\n    instance = value['conf_instance_no']\n    if np.isnan(instance):\n        continue\n    img_path = f\"{DATA_PATH}/{SET_TYPE}_images/{study_id}/{series_id}/{instance}.dcm\"\n    temp = get_image(img_path)\n    \n    set_type = study_id_meta[int(study_id)]['series_descriptions'][\n                    study_id_meta[int(study_id)]['series_id_files'].index(int(series_id))\n    ]\n    \n    # TODO update image path and all that other good stuff here\n    value['img_path'] = img_path\n    value['study_id'] = study_id\n    value['series_id'] = series_id\n    value['set_type'] = set_type\n\n    value['roi_xyxy'] = rescale_bbox(np.array(value['roi_xyxy']), (temp.Rows, temp.Columns))\n    value['conf_xyxy'] = rescale_bbox(np.array(value['conf_xyxy']), (temp.Rows, temp.Columns))\n    ","metadata":{"execution":{"iopub.status.busy":"2024-11-03T04:39:31.010504Z","iopub.execute_input":"2024-11-03T04:39:31.010861Z","iopub.status.idle":"2024-11-03T04:41:31.142186Z","shell.execute_reply.started":"2024-11-03T04:39:31.010833Z","shell.execute_reply":"2024-11-03T04:41:31.141381Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle","metadata":{"execution":{"iopub.status.busy":"2024-11-03T04:42:43.284637Z","iopub.execute_input":"2024-11-03T04:42:43.285034Z","iopub.status.idle":"2024-11-03T04:42:43.289598Z","shell.execute_reply.started":"2024-11-03T04:42:43.285001Z","shell.execute_reply":"2024-11-03T04:42:43.288571Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(f'/kaggle/working/{SET_TYPE}_sagittal_boxes.pkl', 'wb') as file:\n    pickle.dump(new_data, file)","metadata":{"execution":{"iopub.status.busy":"2024-11-03T04:42:43.454990Z","iopub.execute_input":"2024-11-03T04:42:43.455395Z","iopub.status.idle":"2024-11-03T04:42:52.580957Z","shell.execute_reply.started":"2024-11-03T04:42:43.455357Z","shell.execute_reply":"2024-11-03T04:42:52.579907Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}